{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.012565181881007727,
  "eval_steps": 500,
  "global_step": 1100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00011422892619097934,
      "grad_norm": 0.23230332136154175,
      "learning_rate": 0.00019999466928633167,
      "loss": 6.1837,
      "step": 10
    },
    {
      "epoch": 0.0002284578523819587,
      "grad_norm": 0.15372808277606964,
      "learning_rate": 0.0001999870539810912,
      "loss": 0.2307,
      "step": 20
    },
    {
      "epoch": 0.00034268677857293803,
      "grad_norm": 0.06659597158432007,
      "learning_rate": 0.00019997943867585075,
      "loss": 0.1571,
      "step": 30
    },
    {
      "epoch": 0.0004569157047639174,
      "grad_norm": 0.10565146058797836,
      "learning_rate": 0.00019997182337061026,
      "loss": 0.1494,
      "step": 40
    },
    {
      "epoch": 0.0005711446309548967,
      "grad_norm": 0.1852594017982483,
      "learning_rate": 0.0001999642080653698,
      "loss": 0.1419,
      "step": 50
    },
    {
      "epoch": 0.0006853735571458761,
      "grad_norm": 0.27944231033325195,
      "learning_rate": 0.00019995659276012933,
      "loss": 0.1405,
      "step": 60
    },
    {
      "epoch": 0.0007996024833368554,
      "grad_norm": 0.02812562882900238,
      "learning_rate": 0.00019994897745488884,
      "loss": 0.1235,
      "step": 70
    },
    {
      "epoch": 0.0009138314095278348,
      "grad_norm": 0.03361944481730461,
      "learning_rate": 0.00019994136214964838,
      "loss": 0.1204,
      "step": 80
    },
    {
      "epoch": 0.001028060335718814,
      "grad_norm": 0.0303356871008873,
      "learning_rate": 0.00019993374684440792,
      "loss": 0.1211,
      "step": 90
    },
    {
      "epoch": 0.0011422892619097934,
      "grad_norm": 0.031573958694934845,
      "learning_rate": 0.00019992613153916743,
      "loss": 0.1186,
      "step": 100
    },
    {
      "epoch": 0.0012565181881007727,
      "grad_norm": 0.03450406342744827,
      "learning_rate": 0.00019991851623392696,
      "loss": 0.1254,
      "step": 110
    },
    {
      "epoch": 0.0013707471142917521,
      "grad_norm": 0.03765413910150528,
      "learning_rate": 0.00019991090092868647,
      "loss": 0.1235,
      "step": 120
    },
    {
      "epoch": 0.0014849760404827314,
      "grad_norm": 0.03407374769449234,
      "learning_rate": 0.000199903285623446,
      "loss": 0.1216,
      "step": 130
    },
    {
      "epoch": 0.0015992049666737108,
      "grad_norm": 0.03204045817255974,
      "learning_rate": 0.00019989567031820552,
      "loss": 0.122,
      "step": 140
    },
    {
      "epoch": 0.00171343389286469,
      "grad_norm": 0.03003961779177189,
      "learning_rate": 0.00019988805501296506,
      "loss": 0.1175,
      "step": 150
    },
    {
      "epoch": 0.0018276628190556695,
      "grad_norm": 0.033368755131959915,
      "learning_rate": 0.0001998804397077246,
      "loss": 0.1168,
      "step": 160
    },
    {
      "epoch": 0.0019418917452466487,
      "grad_norm": 0.02972550317645073,
      "learning_rate": 0.0001998728244024841,
      "loss": 0.1148,
      "step": 170
    },
    {
      "epoch": 0.002056120671437628,
      "grad_norm": 0.03614198416471481,
      "learning_rate": 0.00019986520909724364,
      "loss": 0.1223,
      "step": 180
    },
    {
      "epoch": 0.0021703495976286074,
      "grad_norm": 0.0368620939552784,
      "learning_rate": 0.00019985759379200318,
      "loss": 0.1172,
      "step": 190
    },
    {
      "epoch": 0.002284578523819587,
      "grad_norm": 0.03821748122572899,
      "learning_rate": 0.0001998499784867627,
      "loss": 0.1212,
      "step": 200
    },
    {
      "epoch": 0.0023988074500105663,
      "grad_norm": 0.030552828684449196,
      "learning_rate": 0.00019984236318152223,
      "loss": 0.1128,
      "step": 210
    },
    {
      "epoch": 0.0025130363762015454,
      "grad_norm": 0.03455088287591934,
      "learning_rate": 0.00019983474787628176,
      "loss": 0.1258,
      "step": 220
    },
    {
      "epoch": 0.002627265302392525,
      "grad_norm": 0.038225192576646805,
      "learning_rate": 0.0001998271325710413,
      "loss": 0.1198,
      "step": 230
    },
    {
      "epoch": 0.0027414942285835043,
      "grad_norm": 0.04053531587123871,
      "learning_rate": 0.0001998195172658008,
      "loss": 0.1184,
      "step": 240
    },
    {
      "epoch": 0.0028557231547744837,
      "grad_norm": 0.03988737612962723,
      "learning_rate": 0.00019981190196056035,
      "loss": 0.1162,
      "step": 250
    },
    {
      "epoch": 0.0029699520809654627,
      "grad_norm": 0.03906776383519173,
      "learning_rate": 0.00019980428665531988,
      "loss": 0.1208,
      "step": 260
    },
    {
      "epoch": 0.003084181007156442,
      "grad_norm": 0.03235629200935364,
      "learning_rate": 0.0001997966713500794,
      "loss": 0.1154,
      "step": 270
    },
    {
      "epoch": 0.0031984099333474216,
      "grad_norm": 0.03940150514245033,
      "learning_rate": 0.00019978905604483893,
      "loss": 0.1189,
      "step": 280
    },
    {
      "epoch": 0.003312638859538401,
      "grad_norm": 0.032637160271406174,
      "learning_rate": 0.00019978144073959847,
      "loss": 0.1198,
      "step": 290
    },
    {
      "epoch": 0.00342686778572938,
      "grad_norm": 0.031493209302425385,
      "learning_rate": 0.00019977382543435798,
      "loss": 0.1139,
      "step": 300
    },
    {
      "epoch": 0.0035410967119203596,
      "grad_norm": 0.032305341213941574,
      "learning_rate": 0.00019976621012911751,
      "loss": 0.1154,
      "step": 310
    },
    {
      "epoch": 0.003655325638111339,
      "grad_norm": 0.03860950842499733,
      "learning_rate": 0.00019975859482387705,
      "loss": 0.1232,
      "step": 320
    },
    {
      "epoch": 0.0037695545643023185,
      "grad_norm": 0.0348476767539978,
      "learning_rate": 0.00019975097951863656,
      "loss": 0.1183,
      "step": 330
    },
    {
      "epoch": 0.0038837834904932975,
      "grad_norm": 0.036192409694194794,
      "learning_rate": 0.0001997433642133961,
      "loss": 0.116,
      "step": 340
    },
    {
      "epoch": 0.003998012416684277,
      "grad_norm": 0.03801894560456276,
      "learning_rate": 0.00019973574890815563,
      "loss": 0.1138,
      "step": 350
    },
    {
      "epoch": 0.004112241342875256,
      "grad_norm": 0.03268067538738251,
      "learning_rate": 0.00019972813360291514,
      "loss": 0.1246,
      "step": 360
    },
    {
      "epoch": 0.004226470269066236,
      "grad_norm": 0.03147878125309944,
      "learning_rate": 0.00019972051829767468,
      "loss": 0.1164,
      "step": 370
    },
    {
      "epoch": 0.004340699195257215,
      "grad_norm": 0.036162253469228745,
      "learning_rate": 0.00019971290299243422,
      "loss": 0.1197,
      "step": 380
    },
    {
      "epoch": 0.004454928121448195,
      "grad_norm": 0.03537429869174957,
      "learning_rate": 0.00019970528768719373,
      "loss": 0.1204,
      "step": 390
    },
    {
      "epoch": 0.004569157047639174,
      "grad_norm": 0.03520718216896057,
      "learning_rate": 0.00019969767238195324,
      "loss": 0.1167,
      "step": 400
    },
    {
      "epoch": 0.004683385973830153,
      "grad_norm": 0.0337471142411232,
      "learning_rate": 0.00019969005707671278,
      "loss": 0.1175,
      "step": 410
    },
    {
      "epoch": 0.004797614900021133,
      "grad_norm": 0.033180855214595795,
      "learning_rate": 0.0001996824417714723,
      "loss": 0.12,
      "step": 420
    },
    {
      "epoch": 0.004911843826212112,
      "grad_norm": 0.0381656251847744,
      "learning_rate": 0.00019967482646623182,
      "loss": 0.1151,
      "step": 430
    },
    {
      "epoch": 0.005026072752403091,
      "grad_norm": 0.036957819014787674,
      "learning_rate": 0.00019966721116099136,
      "loss": 0.1183,
      "step": 440
    },
    {
      "epoch": 0.005140301678594071,
      "grad_norm": 0.03934662416577339,
      "learning_rate": 0.0001996595958557509,
      "loss": 0.1169,
      "step": 450
    },
    {
      "epoch": 0.00525453060478505,
      "grad_norm": 0.035211026668548584,
      "learning_rate": 0.00019965198055051043,
      "loss": 0.1128,
      "step": 460
    },
    {
      "epoch": 0.005368759530976029,
      "grad_norm": 0.037605542689561844,
      "learning_rate": 0.00019964436524526994,
      "loss": 0.1198,
      "step": 470
    },
    {
      "epoch": 0.0054829884571670085,
      "grad_norm": 0.03323432058095932,
      "learning_rate": 0.00019963674994002948,
      "loss": 0.1139,
      "step": 480
    },
    {
      "epoch": 0.0055972173833579875,
      "grad_norm": 0.03442580625414848,
      "learning_rate": 0.00019962913463478902,
      "loss": 0.1144,
      "step": 490
    },
    {
      "epoch": 0.005711446309548967,
      "grad_norm": 0.03653831034898758,
      "learning_rate": 0.00019962151932954853,
      "loss": 0.1198,
      "step": 500
    },
    {
      "epoch": 0.0058256752357399464,
      "grad_norm": 0.037711840122938156,
      "learning_rate": 0.00019961390402430806,
      "loss": 0.1164,
      "step": 510
    },
    {
      "epoch": 0.0059399041619309255,
      "grad_norm": 0.03737369552254677,
      "learning_rate": 0.0001996062887190676,
      "loss": 0.1192,
      "step": 520
    },
    {
      "epoch": 0.006054133088121905,
      "grad_norm": 0.03803252428770065,
      "learning_rate": 0.0001995986734138271,
      "loss": 0.1239,
      "step": 530
    },
    {
      "epoch": 0.006168362014312884,
      "grad_norm": 0.03257902339100838,
      "learning_rate": 0.00019959105810858665,
      "loss": 0.1186,
      "step": 540
    },
    {
      "epoch": 0.006282590940503863,
      "grad_norm": 0.034614600241184235,
      "learning_rate": 0.00019958344280334619,
      "loss": 0.1208,
      "step": 550
    },
    {
      "epoch": 0.006396819866694843,
      "grad_norm": 0.04170682281255722,
      "learning_rate": 0.0001995758274981057,
      "loss": 0.1206,
      "step": 560
    },
    {
      "epoch": 0.006511048792885822,
      "grad_norm": 0.03580575808882713,
      "learning_rate": 0.00019956821219286523,
      "loss": 0.118,
      "step": 570
    },
    {
      "epoch": 0.006625277719076802,
      "grad_norm": 0.03494154289364815,
      "learning_rate": 0.00019956059688762477,
      "loss": 0.1165,
      "step": 580
    },
    {
      "epoch": 0.006739506645267781,
      "grad_norm": 0.03844467177987099,
      "learning_rate": 0.00019955298158238428,
      "loss": 0.1208,
      "step": 590
    },
    {
      "epoch": 0.00685373557145876,
      "grad_norm": 0.03577673062682152,
      "learning_rate": 0.00019954536627714382,
      "loss": 0.1171,
      "step": 600
    },
    {
      "epoch": 0.00696796449764974,
      "grad_norm": 0.03311630338430405,
      "learning_rate": 0.00019953775097190335,
      "loss": 0.1176,
      "step": 610
    },
    {
      "epoch": 0.007082193423840719,
      "grad_norm": 0.03501208499073982,
      "learning_rate": 0.00019953013566666286,
      "loss": 0.1165,
      "step": 620
    },
    {
      "epoch": 0.007196422350031698,
      "grad_norm": 0.03866155073046684,
      "learning_rate": 0.0001995225203614224,
      "loss": 0.1161,
      "step": 630
    },
    {
      "epoch": 0.007310651276222678,
      "grad_norm": 0.0343233197927475,
      "learning_rate": 0.00019951490505618194,
      "loss": 0.1175,
      "step": 640
    },
    {
      "epoch": 0.007424880202413657,
      "grad_norm": 0.033536918461322784,
      "learning_rate": 0.00019950728975094147,
      "loss": 0.114,
      "step": 650
    },
    {
      "epoch": 0.007539109128604637,
      "grad_norm": 0.033374104648828506,
      "learning_rate": 0.00019949967444570098,
      "loss": 0.115,
      "step": 660
    },
    {
      "epoch": 0.007653338054795616,
      "grad_norm": 0.0333176888525486,
      "learning_rate": 0.0001994920591404605,
      "loss": 0.1147,
      "step": 670
    },
    {
      "epoch": 0.007767566980986595,
      "grad_norm": 0.0334494411945343,
      "learning_rate": 0.00019948444383522003,
      "loss": 0.1099,
      "step": 680
    },
    {
      "epoch": 0.007881795907177575,
      "grad_norm": 0.03510277718305588,
      "learning_rate": 0.00019947682852997954,
      "loss": 0.1177,
      "step": 690
    },
    {
      "epoch": 0.007996024833368554,
      "grad_norm": 0.02942696027457714,
      "learning_rate": 0.00019946921322473908,
      "loss": 0.1144,
      "step": 700
    },
    {
      "epoch": 0.008110253759559533,
      "grad_norm": 0.03348400443792343,
      "learning_rate": 0.00019946159791949861,
      "loss": 0.1188,
      "step": 710
    },
    {
      "epoch": 0.008224482685750512,
      "grad_norm": 0.033567193895578384,
      "learning_rate": 0.00019945398261425815,
      "loss": 0.1163,
      "step": 720
    },
    {
      "epoch": 0.008338711611941493,
      "grad_norm": 0.03290293738245964,
      "learning_rate": 0.00019944636730901766,
      "loss": 0.1149,
      "step": 730
    },
    {
      "epoch": 0.008452940538132472,
      "grad_norm": 0.033886052668094635,
      "learning_rate": 0.0001994387520037772,
      "loss": 0.1161,
      "step": 740
    },
    {
      "epoch": 0.00856716946432345,
      "grad_norm": 0.035939477384090424,
      "learning_rate": 0.00019943113669853674,
      "loss": 0.1141,
      "step": 750
    },
    {
      "epoch": 0.00868139839051443,
      "grad_norm": 0.032728057354688644,
      "learning_rate": 0.00019942352139329625,
      "loss": 0.1129,
      "step": 760
    },
    {
      "epoch": 0.008795627316705409,
      "grad_norm": 0.03382628783583641,
      "learning_rate": 0.00019941590608805578,
      "loss": 0.1145,
      "step": 770
    },
    {
      "epoch": 0.00890985624289639,
      "grad_norm": 0.03356998413801193,
      "learning_rate": 0.00019940829078281532,
      "loss": 0.119,
      "step": 780
    },
    {
      "epoch": 0.009024085169087369,
      "grad_norm": 0.03810093551874161,
      "learning_rate": 0.00019940067547757483,
      "loss": 0.1228,
      "step": 790
    },
    {
      "epoch": 0.009138314095278348,
      "grad_norm": 0.03326253592967987,
      "learning_rate": 0.00019939306017233437,
      "loss": 0.1165,
      "step": 800
    },
    {
      "epoch": 0.009252543021469327,
      "grad_norm": 0.033141158521175385,
      "learning_rate": 0.0001993854448670939,
      "loss": 0.1143,
      "step": 810
    },
    {
      "epoch": 0.009366771947660306,
      "grad_norm": 0.03146011754870415,
      "learning_rate": 0.0001993778295618534,
      "loss": 0.1168,
      "step": 820
    },
    {
      "epoch": 0.009481000873851285,
      "grad_norm": 0.03353019058704376,
      "learning_rate": 0.00019937021425661295,
      "loss": 0.1151,
      "step": 830
    },
    {
      "epoch": 0.009595229800042265,
      "grad_norm": 0.03795850649476051,
      "learning_rate": 0.0001993625989513725,
      "loss": 0.1155,
      "step": 840
    },
    {
      "epoch": 0.009709458726233244,
      "grad_norm": 0.03716360032558441,
      "learning_rate": 0.000199354983646132,
      "loss": 0.1196,
      "step": 850
    },
    {
      "epoch": 0.009823687652424223,
      "grad_norm": 0.03363899886608124,
      "learning_rate": 0.00019934736834089153,
      "loss": 0.1153,
      "step": 860
    },
    {
      "epoch": 0.009937916578615202,
      "grad_norm": 0.04196134954690933,
      "learning_rate": 0.00019933975303565107,
      "loss": 0.1167,
      "step": 870
    },
    {
      "epoch": 0.010052145504806181,
      "grad_norm": 0.03664488345384598,
      "learning_rate": 0.0001993321377304106,
      "loss": 0.1147,
      "step": 880
    },
    {
      "epoch": 0.010166374430997162,
      "grad_norm": 0.031161881983280182,
      "learning_rate": 0.00019932452242517012,
      "loss": 0.1182,
      "step": 890
    },
    {
      "epoch": 0.010280603357188141,
      "grad_norm": 0.04538947343826294,
      "learning_rate": 0.00019931690711992966,
      "loss": 0.1202,
      "step": 900
    },
    {
      "epoch": 0.01039483228337912,
      "grad_norm": 0.033236343413591385,
      "learning_rate": 0.0001993092918146892,
      "loss": 0.1139,
      "step": 910
    },
    {
      "epoch": 0.0105090612095701,
      "grad_norm": 0.03573683649301529,
      "learning_rate": 0.0001993016765094487,
      "loss": 0.1102,
      "step": 920
    },
    {
      "epoch": 0.010623290135761078,
      "grad_norm": 0.03486509248614311,
      "learning_rate": 0.00019929406120420824,
      "loss": 0.118,
      "step": 930
    },
    {
      "epoch": 0.010737519061952057,
      "grad_norm": 0.033843379467725754,
      "learning_rate": 0.00019928644589896775,
      "loss": 0.1167,
      "step": 940
    },
    {
      "epoch": 0.010851747988143038,
      "grad_norm": 0.03303895518183708,
      "learning_rate": 0.00019927883059372729,
      "loss": 0.1145,
      "step": 950
    },
    {
      "epoch": 0.010965976914334017,
      "grad_norm": 0.03474811837077141,
      "learning_rate": 0.0001992712152884868,
      "loss": 0.1185,
      "step": 960
    },
    {
      "epoch": 0.011080205840524996,
      "grad_norm": 0.03567845746874809,
      "learning_rate": 0.00019926359998324633,
      "loss": 0.1113,
      "step": 970
    },
    {
      "epoch": 0.011194434766715975,
      "grad_norm": 0.03106958605349064,
      "learning_rate": 0.00019925598467800587,
      "loss": 0.1135,
      "step": 980
    },
    {
      "epoch": 0.011308663692906954,
      "grad_norm": 0.03409494459629059,
      "learning_rate": 0.00019924836937276538,
      "loss": 0.1201,
      "step": 990
    },
    {
      "epoch": 0.011422892619097935,
      "grad_norm": 0.03504657745361328,
      "learning_rate": 0.00019924075406752492,
      "loss": 0.1099,
      "step": 1000
    },
    {
      "epoch": 0.011537121545288914,
      "grad_norm": 0.033827897161245346,
      "learning_rate": 0.00019923313876228445,
      "loss": 0.1158,
      "step": 1010
    },
    {
      "epoch": 0.011651350471479893,
      "grad_norm": 0.029607368633151054,
      "learning_rate": 0.00019922552345704396,
      "loss": 0.1182,
      "step": 1020
    },
    {
      "epoch": 0.011765579397670872,
      "grad_norm": 0.035104747861623764,
      "learning_rate": 0.0001992179081518035,
      "loss": 0.1121,
      "step": 1030
    },
    {
      "epoch": 0.011879808323861851,
      "grad_norm": 0.035445354878902435,
      "learning_rate": 0.00019921029284656304,
      "loss": 0.1141,
      "step": 1040
    },
    {
      "epoch": 0.011994037250052832,
      "grad_norm": 0.030789190903306007,
      "learning_rate": 0.00019920267754132255,
      "loss": 0.1115,
      "step": 1050
    },
    {
      "epoch": 0.01210826617624381,
      "grad_norm": 0.03267129510641098,
      "learning_rate": 0.00019919506223608208,
      "loss": 0.1137,
      "step": 1060
    },
    {
      "epoch": 0.01222249510243479,
      "grad_norm": 0.038477230817079544,
      "learning_rate": 0.00019918744693084162,
      "loss": 0.1186,
      "step": 1070
    },
    {
      "epoch": 0.012336724028625769,
      "grad_norm": 0.032944079488515854,
      "learning_rate": 0.00019917983162560113,
      "loss": 0.1202,
      "step": 1080
    },
    {
      "epoch": 0.012450952954816748,
      "grad_norm": 0.032631512731313705,
      "learning_rate": 0.00019917221632036067,
      "loss": 0.1185,
      "step": 1090
    },
    {
      "epoch": 0.012565181881007727,
      "grad_norm": 0.03440256789326668,
      "learning_rate": 0.0001991646010151202,
      "loss": 0.1156,
      "step": 1100
    }
  ],
  "logging_steps": 10,
  "max_steps": 262629,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5166792947204096e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
