{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "# This must be first\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv(\".env\")\n",
                "\n",
                "import os\n",
                "import string\n",
                "import pickle\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "from core.utils.misc import seed_everything\n",
                "from tqdm.auto import tqdm"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from core.utils.misc import limit_gpus\n",
                "from core.models.llm_loading import load_model_and_tokenizer, load_tokenizer\n",
                "\n",
                "limit_gpus(range(0, 8))\n",
                "\n",
                "# model_type, model_variant = \"gpt-2\", \"1.5B\"\n",
                "model_type, model_variant = \"llama\", \"7B\"\n",
                "# model_type, model_variant = \"gpt-j\", \"6B\"\n",
                "# model_type, model_variant = \"pythia\", \"2.8B\"\n",
                "# model_type, model_variant = \"pythia\", \"6.9B\"\n",
                "# model_type, model_variant = \"falcon\", \"7B\"\n",
                "# model_type, model_variant = \"mpt\", \"7B\"\n",
                "\n",
                "model, tokenizer = load_model_and_tokenizer(model_type, model_variant)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# In Context Learning Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main Experiment\n",
                "\n",
                "from core.task_vectors import run_icl, run_task_vector\n",
                "from core.data.task_helpers import get_task_by_name\n",
                "from core.analysis.evaluation import calculate_accuracy, calculate_accuracy_on_datasets, print_evaluation_summary\n",
                "\n",
                "seed_everything(41)\n",
                "\n",
                "# task_name = \"knowledge_country_capital\"\n",
                "# task_name = \"knowledge_person_language\"\n",
                "# task_name = \"knowledge_location_continent\"\n",
                "# task_name = \"knowledge_location_religion\"\n",
                "\n",
                "# task_name = \"algorithmic_prev_letter\"  # 0.57,0.40\n",
                "# task_name = \"algorithmic_next_letter\"  # 0.91, 0.94\n",
                "# task_name = \"algorithmic_list_first\"  # 1.00, 0.99\n",
                "# task_name = \"algorithmic_list_last\"  # 0.97, 0.86\n",
                "# task_name = \"algorithmic_to_upper\"  # 1.00, 0.96\n",
                "# task_name = \"algorithmic_to_lower\"  # 1.00, 0.88\n",
                "\n",
                "task_name = \"translation_fr_en\"\n",
                "# task_name = \"translation_es_en\"\n",
                "# task_name = \"translation_it_en\" \n",
                "# task_name = \"translation_en_fr\"\n",
                "# task_name = \"translation_en_es\"\n",
                "# task_name = \"translation_en_it\"\n",
                "\n",
                "# task_name = \"linguistic_present_simple_gerund\"  # 0.96, 0.80\n",
                "# task_name = \"linguistic_present_simple_past_simple\"  # 0.95, 0.94\n",
                "# task_name = \"linguistic_present_simple_past_perfect\"  # 0.79, 0.61\n",
                "# task_name = \"linguistic_plural_singular\"  # 0.90, 0.81\n",
                "# task_name = \"linguistic_antonyms\"  # 0.90, 0.88\n",
                "\n",
                "# task_name = \"sentiment\"\n",
                "\n",
                "num_examples = 5\n",
                "\n",
                "task = get_task_by_name(tokenizer, task_name)\n",
                "\n",
                "test_datasets = task.create_datasets(num_datasets=100, num_examples=num_examples)\n",
                "dev_datasets = task.create_datasets(num_datasets=50, num_examples=num_examples)\n",
                "\n",
                "icl_predictions = run_icl(model, tokenizer, task, test_datasets)\n",
                "print_evaluation_summary(task, icl_predictions, test_datasets)\n",
                "\n",
                "tv_predictions, tv_dev_accuracy_by_layer, task_hiddens = run_task_vector(\n",
                "    model,\n",
                "    tokenizer,\n",
                "    task,\n",
                "    test_datasets,\n",
                "    dev_datasets,\n",
                "    multi_context=False\n",
                ")\n",
                "print_evaluation_summary(task, tv_predictions, test_datasets)\n",
                "# print(tv_dev_accuracy_by_layer)\n",
                "\n",
                "# icl_accuracy = calculate_accuracy_on_datasets(icl_predictions, test_datasets)\n",
                "# tv_accuracy = calculate_accuracy_on_datasets(tv_predictions, test_datasets)\n",
                "# print(f\"ICL accuracy: {icl_accuracy:.2f}\")\n",
                "# print(f\"TV accuracy: {tv_accuracy:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Overriding Experiment\n",
                "\n",
                "from typing import Any\n",
                "from core.task_vectors import run_icl, run_overriding_task_vector\n",
                "from core.data.task_helpers import get_task_by_name\n",
                "from core.analysis.evaluation import calculate_accuracy\n",
                "from core.data.tasks.task import Task\n",
                "from scripts.experiments.overriding import OVERRIDING_TASK_PAIRS\n",
                "\n",
                "def is_valid_input(task: Task, inp: Any) -> bool:\n",
                "    try:\n",
                "        task.calc_output(inp)\n",
                "        return True\n",
                "    except:\n",
                "        return False\n",
                "\n",
                "seed_everything(41)\n",
                "\n",
                "\n",
                "task_name, overriding_task_name = OVERRIDING_TASK_PAIRS[3]\n",
                "\n",
                "num_examples = 4\n",
                "\n",
                "task = get_task_by_name(tokenizer, task_name)\n",
                "overriding_task = get_task_by_name(tokenizer, overriding_task_name)\n",
                "\n",
                "test_datasets = task.create_datasets(num_datasets=1000, num_examples=num_examples)\n",
                "overriding_datasets = overriding_task.create_datasets(num_datasets=100, num_examples=num_examples)\n",
                "\n",
                "# filter only test_datasets that are valid inputs for the overriding task\n",
                "test_datasets = [dataset for dataset in test_datasets if is_valid_input(overriding_task, dataset.test_input)]\n",
                "test_datasets = test_datasets[:len(overriding_datasets)]\n",
                "\n",
                "assert len(test_datasets) == len(overriding_datasets)\n",
                "\n",
                "icl_predictions = run_icl(model, tokenizer, task, test_datasets)\n",
                "tv_predictions, tv_dev_accuracy_by_layer, task_hiddens = run_overriding_task_vector(\n",
                "    model,\n",
                "    tokenizer,\n",
                "    task,\n",
                "    test_datasets,\n",
                "    overriding_datasets,\n",
                "    verbose=True,\n",
                ")\n",
                "\n",
                "expected_outputs_original = [dataset.test_output for dataset in test_datasets]\n",
                "expected_outputs_patched = [overriding_task.calc_output(dataset.test_input) for dataset in test_datasets]\n",
                "\n",
                "icl_accuracy_original = calculate_accuracy(task,icl_predictions, expected_outputs_original)\n",
                "icl_accuracy_patched = calculate_accuracy(task,icl_predictions, expected_outputs_patched)\n",
                "\n",
                "tv_accuracy_original = calculate_accuracy(task,tv_predictions, expected_outputs_original)\n",
                "tv_accuracy_patched = calculate_accuracy(task,tv_predictions, expected_outputs_patched)\n",
                "\n",
                "print(f\"ICL accuracy original: {icl_accuracy_original:.2f}\")\n",
                "print(f\"ICL accuracy patched: {icl_accuracy_patched:.2f}\")\n",
                "print(f\"TV accuracy original: {tv_accuracy_original:.2f}\")\n",
                "print(f\"TV accuracy patched: {tv_accuracy_patched:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top tokens\n",
                "\n",
                "from core.analysis.utils import logits_top_tokens, tokens_ranks\n",
                "from core.models.utils.inference import hidden_to_logits\n",
                "from itertools import chain\n",
                "\n",
                "tv_ordered_tokens_by_layer = {}\n",
                "\n",
                "for layer_num in tv_dev_accuracy_by_layer.keys():\n",
                "    task_hidden = task_hiddens.mean(axis=0)[layer_num]\n",
                "    logits = hidden_to_logits(model, task_hidden)\n",
                "    tv_ordered_tokens_by_layer[layer_num] = logits_top_tokens(logits, tokenizer, k=500)\n",
                "    print(\"Top tokens for layer\", layer_num, \":\", tv_ordered_tokens_by_layer[layer_num][:12])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if input+task=output\n",
                "\n",
                "from core.models.utils.llm_layers import get_lm_head, get_lm_pipeline\n",
                "from core.models.utils.inference import hidden_to_logits, logits_to_tokens\n",
                "\n",
                "# Find the layer that has the highest accuracy\n",
                "layer_num = max(tv_dev_accuracy_by_layer, key=tv_dev_accuracy_by_layer.get) + 3\n",
                "\n",
                "task_hidden = task_hiddens.mean(axis=0)[layer_num]\n",
                "\n",
                "embeddings = get_lm_head(model).weight.float().cpu()\n",
                "\n",
                "inputs = [dataset.test_input.strip() for dataset in test_datasets]\n",
                "\n",
                "inputs_token_ids = tokenizer(inputs, add_special_tokens=False).input_ids\n",
                "\n",
                "inputs_token_ids = [x[0] for x in inputs_token_ids if len(x) == 1]\n",
                "\n",
                "input_token_embeddings = embeddings[inputs_token_ids].cpu()\n",
                "\n",
                "# normalize embeddings and task_hidden\n",
                "input_token_embeddings = input_token_embeddings / input_token_embeddings.norm(dim=-1, keepdim=True)\n",
                "task_hidden = task_hidden / task_hidden.norm(dim=-1, keepdim=True)\n",
                "\n",
                "input_plus_task_embeddings = input_token_embeddings + task_hidden * 1.0\n",
                "\n",
                "logits = hidden_to_logits(model, input_plus_task_embeddings)\n",
                "# logits = embeddings @ input_plus_task_embeddings.T\n",
                "\n",
                "ignore_ids=inputs_token_ids\n",
                "# ignore_ids=None\n",
                "\n",
                "outputs = logits_to_tokens(logits, tokenizer, ignore_ids=ignore_ids)\n",
                "\n",
                "list(zip(inputs, outputs))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from core.models.utils.inference import batch_generate, tokenize_prompts\n",
                "\n",
                "from transformers import TextGenerationPipeline\n",
                "\n",
                "prompt_examples = [\n",
                "    \"Canada -> Ottawa\",\n",
                "    # \"Australia -> Canberra\",\n",
                "    \"France -> Paris\",\n",
                "    \"Germany -> Berlin\",\n",
                "    # \"Australia -> Sydney\",\n",
                "    \"Switzerland ->\",\n",
                "    # \"India -> Mumbai\",\n",
                "    # \"China -> Shanghai\",\n",
                "    # \"Australia ->\"\n",
                "]\n",
                "\n",
                "prompt = \"\\n\".join(prompt_examples)\n",
                "\n",
                "pipeline = TextGenerationPipeline(model, tokenizer)\n",
                "\n",
                "completion = pipeline(prompt, max_new_tokens=2, num_return_sequences=1, do_sample=False)\n",
                "\n",
                "print(completion)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}